\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pdfpages}
\usepackage[hidelinks]{hyperref}
\usepackage{subcaption}
\usepackage{float}
\usepackage{natbib}
\usepackage{doi}



\title{On the Periodic Behavior of DNNs training on the example of
the Grokking effect}

\author{ Мельник Ю. М. \\
	Кафедра ММП, факультет ВМК \\
    МГУ им. М.В. Ломоносова\\
	\texttt{melnik.um@yandex.ru} \\
	%% examples of more authors
	\And
    Южаков Т. А.\\
	ФКН, НИУ ВШЭ\\
    Исследовательская группа Байесовских методов\\
    \And
    Ветров Д.П.\\
    кандидат ф.-м. наук\\
    Профессор НИУ ВШЭ\\
    Исследовательская группа Байесовских методов\\
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

%%\renewcommand{\shorttitle}{\textit }

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\begin{document}
\maketitle

\begin{abstract}
	Глубокие нейронные сети часто обучаются с использованием слоёв нормализации, что позволяет стабилизировать процесс обучения и повысить точность предсказания модели. Однако иногда использование подобных техник в совокупности с применением методов регуляризации может привести к возникновению необычных эффектов при обучении нейросетей. В данной работе приведены результаты исследования одного из таких эффектов, а именно \verb|периодического поведения|, которое выражается в том, что в процессе обучения нейросети изменение значений лосс функции и основных метрик происходит по определённому повторяющемуся шаблону. На примере эффекта <<\verb|гроккинга|>> - явления, связанного с переобучением нейросетевых, в частности, трансформерных моделей - было показано, что периодического поведения при обучении глубоких нейросетей можно добиться путём использования \verb|масштабно-инвариантных| архитектур. Также была установлена связь между возникновением периодического поведения и изменением значений \verb|эффективного темпа обучения| и средней нормы \verb|эффективного градиента| в процессе обучения модели.  
\end{abstract}

\keywords{Grokking \and Loss Landscape  \and Weight Decay \and Scale-Invariance }

\section{Введение}
Обобщение перепараметризованных нейронных сетей уже давно является источником интереса для
сообщества машинного обучения, поскольку оно бросает вызов интуиции, вытекающей из классической теории обучения \cite{double_descent, dd_avoid}. Так, в статье  \cite{on_periodic_beh} описываются результаты использования слоёв нормализации \verb|BatchNorm| \cite{batch_norm} вместе с регуляризатором <<weight decay>> \cite{weight_decay, decoupled_wd, gradient_reg} и возникающее в процессе обучения нейросети периодическое поведение, которое заключается в том, что изменение значений лосс функции и основных метрик происходит по определённому повторяющемуся шаблону. Другим же необычным эффектом, возникающим при обучении перепараметризованных нейросетей, является эффект <<гроккинга>>, впервые обнаруженный авторами статьи \cite{grokking_start}. Исследователями из OpenAI было показано, что обучаемые на небольших алгоритмически сгенерированных наборах данных сети могут демонстрировать необычные шаблоны обобщения, явно не связанные с качеством на обучающей выборке: если продолжить обучать переобученную модель, то спустя некоторое достаточно большое время это приведёт к росту точности на отложенной выборке. В других работах по изучению эффекта гроккинга авторы делятся своей интуицией по поводу причин его возникновения. Например, в статье \cite{phase_changes} эффект гроккинга рассматривает через призму <<фазовых переходов>> от запоминания выборки нейросетью до выучивания закономерностей в данных. В статье же \cite{lazy_grokking} говорится, что эффект гроккинга не что иное, как переход от "ленивой" динамики обучения к стадии "быстрого" выучивания закономерностей в обучающей выборке. Авторы же другой статьи \cite{grokking_tickets} рассматривают эффект гроккинга в контексте так называемого феномена <<золотого билета>>. В данной же работе будут высказаны идеи, отличные от выше описанных, которые, однако, во многом опираются на результаты, полученные авторами статей \cite{grokking_start, omnigrok}. \\\\
Итак, эффект гроккинга можно разбить на два основных этапа:
\begin{itemize}
    \item точность на обучающей выборке равно 100\%, при этом соответствующее значение для отложенной выборки близко к нулю (этап запоминания обучающей выборки, то есть переобучение)
    \item значение точности на обучающей и отложенной выборках равно 100\% (этап выучивания закономерностей в данных, то есть генерализация)
\end{itemize}
\par Далее будем называть состояние модели, при котором достигается 100\% точности на обучающей выборке, <<точкой 1>>, а состояние, при котором точность на трейне и на валидации достигает 100\% одновременно, <<точкой 2>> (точки пронумерованы в соответствие с хронологией обучения модели). На графиках ниже показано типичное поведение точности и значения функции потерь на обучающей и валидационной выборках при наблюдении эффекта гроккинга.
\begin{figure}[h]
\centering
\subfloat[График точности]{\includegraphics[width = 0.5\textwidth]{grokking_1l_acc.pdf}}
\subfloat[График значений функции потерь]{\includegraphics[width = 0.5\textwidth]{grokking_1l_loss.pdf}}
\caption{Графики точности и значения функции потерь, типичные для эффекта гроккинга}
\label{fig:fig0}
\end{figure}
\par В качестве модели авторами оригинальной статьи \cite{grokking_start} была выбрана небольшая трансформерная архитектура (2 слоя ширины 128 с 4 головками внимания), обучавшаяся с помощью оптимизатора AdamW со следующими параметрами: learning rate =  $10^{-3}$, weight decay = 1, $\beta_1$ = 0.9, $\beta_2$ = 0.98, linear learning rate warmup первые 10 шагов оптимизации, размер мини-батча 512.
\par Для обучения нейросетевой модели использовался алгоритмически сгенерированный датасет равенств вида <<a o b = c>>, где <<a>>, <<b>>, <<c>> - целые неотрицательные числа, а <<o>> - некая бинарная операция. 
В качестве бинарной операции <<o>> авторы  использовали бинарные операции по модулю простого числа p, например (x + y) mod p или (x * y) mod p (где x и y - целые неотрицательные числа, непревосходящие p - 1). Важно отметить, что для лучшей воспроизводимости эффекта гроккинга при генерации данных следует выбирать симметричные относительно своих аргументов операции. 
\par Ещё одним важным гиперпараметром модели, влияющим на воспроизводимость гроккинга, является доля обучающей выборки. В зависимости от значения данного параметра точность на валидационной выборке может вести себя по-разному. Крайними ситуациями такого поведения являются нулевые показатели точности (модель переобучилась и генерализация не наступила) и одновременный рост точности на обучающей и валидационной выборках (то есть отсутствие эффекта гроккинга).
 Стоит отметить, что необходимым условием возникновения эффекта гроккинга является использование при обучении нейросетевой модели регуляризатора весов. Данный факт подтверждается результатами экспериментов по анализу поведения нормы весов модели в процессе обучения и согласуется с выводами авторов статей \cite{grokking_start, omnigrok}.

Для понимания причин возникновения периодического поведения в процессе обучения нейросети необходимо определить понятие \textbf{масштабной инвариантности}. Пусть $p(y|x, \theta)$ - предсказание нейросетевой модели. Тогда модель называется 
\textbf{масштаб-инвариантной} по весам, если выполняется следующее соотношение:
$$log \hspace{1mm} p(y|x, \theta) = log \hspace{1mm} p(y|x, C\theta), \hspace{2mm}\forall C > 0$$
\par В сущности это понятие означает, что модель реализует единственную функцию вдоль каждого вектора (в пространстве весов), берущего начало в нуле, вне завивисмости от расстояния до начала координат:
\begin{figure}[h]
\centering
\includegraphics[scale=0.9]{scale_inv_cur.pdf}
\caption{Визуализация масштабной инвариантности модели для случая двумерного пространства весов. F и F' - функции, которые реализует модель вдоль соответствующих лучей. R - радиус малой окружности, С - вещественная положительная константа.}
\label{fig:fig4}
\end{figure}
\section{Постановка задачи}
В рамках воспроизведения эффекта гроккинга решается задача многоклассовой классификации с числом классов $K=97$. В качестве лосс функции была выбрана кросс-энтропийная функция потерь. Пусть $X = \{{x_i}\}_{i=1}^N$, $x_i \in R^2$ - обучающая выборка,  $Y = \{{y_i}\}_{i=1}^N$, $y_i \in \{0, 1, ...,K\}$  $f_{\theta}(x): R^2 \rightarrow R^{97}$ - функция, реализующаяся нейросетью с параметрами $\theta$, $\lambda$ - коэффициент регуляризации. Тогда в рамках решения задачи многоклассовой классификации решается следующая оптимизационная задача:
$$\mathcal{L}(X, Y, \theta) = -\frac{1}{N} \sum_{i=1}^N\sum_{k=0}^K{y_{ik} log{\frac{\exp{f_{\theta}(x_i)_k}}{\sum_{j=0}^K{\exp{f_{\theta}(x_i)_j}}}}} + \frac{\lambda}{2}\|\theta\|^2 \rightarrow\min_{\theta}$$
Данная оптимизационная задача решается с помощью метода Cтохастического градиентного спуска.
\section{Эксперименты}
\subsection{Описание данных}
Все эксперименты в данной работе были проведены на алгоритмически сгенерированном датасете равенств вида <<a o b = c>>, описанном в пункте 1.1, где в качестве бинарной операции было выбрано сложение по модулю 97: (x + y) mod 97. Единственное отличие между данными, использовавшимися в оригинальной статье, и датасетом, использовавшемся в этой работе, заключается в том, что при токенизации равенств <<a o b = c>> не использовались токены для самой бинарной операции <<o>>, а также знака <<=>> (то есть каждый объект состоит только из трёх токенов, соответствующих операндам и результату). Выбор такого способа токенизации продиктован меньшей зашумлённостью процесса обучения, по сравнению с вариантом, предложенным авторами оригинальной статьи.
\subsection{Модель}
В качестве модели для проведения экспериментов была выбрана трансформерная архитектура, описанная в статье \cite{omnigrok}.
\begin{figure}[h]
\centering
\includegraphics[scale=0.9]{transformer_block.pdf}
\caption{Блок трансформера используемой нейросетевой архитектуры}
\label{fig:fig5}
\end{figure}
\par Расшифруем обозначения, использованные на рис.\ref{fig:fig5}:
\begin{itemize}
    \item \verb|LayerNorm| - слой нормализации \cite{layer_norm}
    \item \verb|Attention Layer| - слой внимания (является основой трансформерной архитектуры \cite{attn_is_all})
    \item \verb|MLP| - два полносвязных линейных слоя с \verb|ReLu|  в качестве функции активации после первого из них
    \item Кружок со знаком <<+>> внутри означает \verb|Skip Connection|
\end{itemize}
Для всех последующих экспериментов зафиксируем параметры модели:
\begin{itemize}
    \item Размер словаря (параметр \verb|d_vocab|) равен 97
    \item Размерность скрытого пространства модели (параметр \verb|d_model|) равна 128
    \item Размерность линейного слоя (параметр \verb|d_mpl|) равна 512
    \item Число голов внимания (параметр \verb|num_heads|) равно 4
    \item Размерность скрытого пространства механизма внимания (параметр \verb|d_head|) равна 32
    \item Длина контекста (параметр \verb|n_ctx|) равна 2
\end{itemize}
Если же говорить про значения параметра \verb|num_layers|, который отвечает за число слоёв трансформерной архитектуры, то в данном исследовании будут рассмотрены однослойная и двуслойная модели (\verb|num_layers| = 1 и \verb|num_layers| = 2).
\subsection{Способ обучения}
В рамках воспроизведения эффекта гроккинга решается задача многоклассовой классификации, в которой таргетом является токен <<c>> (результат выполнения бинарной операции (a + b) mode 97 - один из 97 классов), а исходными признаками - токены <<a>> и <<b> (операнды данной бинарной операции). В качестве функционала ошибки для решения данной задачи была выбрана кросс-энтропийная функция потерь.
В отличие от оригинальной статьи в данной работе в качестве оптимизатора во всех экспериментах использовался \verb|SGD| (Stochastic Gradient Descent) \cite{lion} с постоянным темпом обучения с целью упрощения интерпретабельность результатов. В качестве значений параметров оптимизатора были приняты величины: \verb|lr = 0.1| (темп обучения), \verb|weight_decay = 0.001| (значение константы $\lambda$ перед регуляризационным слагаемым). Значения гиперпараметров модели, а именно доли обучающей выборки и размера батча, выберем равными 0.4 и 512 соответственно.
\subsection{Использование нормализации для достижения периодического поведения}
В статье \cite{on_periodic_beh} было показано, что использование слоёв нормализации \verb|BatchNorm| после каждого свёрточного слоя сети приводит к возникновению периодического поведения при обучении модели за счёт приобретения ею свойства \textbf{масштабной инвариантности} \cite{si_attn, 3_rigimes}.
Воспользуемся данной идеей для нашей задачи и попробуем добиться периодического поведения засчёт использования слоёв \verb|LayerNorm| в соответствии со схемой, изображённой на рис.\ref{fig:fig5}.
Помимо блока трансформера, слои нормализации также добавляются и внутрь блока \verb|MLP|, а также перед последним линейным слоем модели, отвечающим за перевод данных из векторного представления (эмбедингов) в токены исходного словаря. 
\par Прежде чем переходить к анализу результатов данного эксперимента стоит сделать важное замечание. Данная модель не является полностью масштаб-инвариантной из-за наличия в ней слоя внимания (\verb|Attention Layer|), поэтому при дальнейшем анализе статистик (раздел 3.2), характерных именно для масштаб-инвариантных сетей, будут сделаны некоторые допущения, которые не умаляют логики рассуждений, а \textbf{частичную масштаб-инвариантность}, свойственную моделям, задействованных в экспериментах, будем называть просто масштаб-инвариантностью для краткости.
\par Глядя на графики точности на обучающей и валидационной выборках (рис. \ref{fig:fig8}), можно заметить что эффекта гроккинга удалось добиться как для однослойной, так и для двуслойной модели. Однако вместе с тем обучение обеих моделей приобрело желаемый периодический характер: на графиках видны просадки значений точности на обучающей выборке, которым соответствуют восходящие скачки точности на валидации:
\begin{figure}[h]
\centering
\subfloat[График точности однослойного трансформера]{\includegraphics[width = 0.5\textwidth]{grokking_1l_acc_ln.pdf}}
\subfloat[График точности двуслойного трансформера трансформера]{\includegraphics[width = 0.5\textwidth]{grokking_2ln.pdf}}
\caption{Графики точности моделей траснформеров с использованием нормализации}
\label{fig:fig8}
\end{figure}
\par Данный эффект можно объяснить приобретением свойства \textbf{масштаб-инвариантности} большинством весов нейронной сети за счёт использования нормализации между слоями модели.
\subsection{Анализ статистик обучения}
Для дальнейшего анализа статистик обучения необходимо ввести некоторые дополнительные понятия, которые бы учитывали масштаб-инвариантность нейросети \cite{sam, asam}. Пусть $\eta$ - темп обучения, $\theta$ - веса модели, $g$ - градиент функции потерь по весам. Тогда определим \textbf{эффективный градиент} и \textbf{эффективный темпа обучения} как:
$$g_{eff} = g \|\theta\|$$
$$\eta_{eff} = \frac{\eta}{\|\theta\|^2}$$
\par В силу масштаб-инвариантности модели процессу оптимизации во всём пространстве весов можно взаимно однозначно сопоставить процесс оптимизации на единичной гиперсфере. Тогда понятия эффективных градиента и темпа обучения означают градиент и темп обучения при оптимизации на единичной гиперсфере.
\par Теперь установим причины периодического поведения в процессе обучения на примере модели двуслойного трансформера , график точности которого представлен на рисунке \ref{fig:fig8}b. Для этого проанализируем поведение эффективного темпа обучения и нормы эффективного градиента в процессе обучения.
\begin{figure}[h]
\centering
\subfloat[График эффективного темпа обучения]{\includegraphics[width = 0.5\textwidth]{grokking_lr.pdf}}
\subfloat[График нормы эффективного градиента]{\includegraphics[width = 0.5\textwidth]{grokking_eff_grad.pdf}}
\caption{Графики эффективных статистик двуслойного трансформера с использованием LayerNorm}
\label{fig:fig9}
\end{figure}
\par Сопоставляя графики на рис. \ref{fig:fig9} и график точности на рис. \ref{fig:fig8}b, можно заметить, что моменты падений точности на обучающей выборке соответствуют моментам <<скачков>> эффективных темпа обучения и нормы градиента. При этом график эффективного темпа обучения имеет <<пиловидный>> характер, что можно объяснить резкими изменениями значения нормы весов модели: <<скачки>> градиента заставляют совершать перемещение по гиперсферам разного радиуса в пространстве весов, а за счёт наличия регуляризатора weight decay норма весов уменьшается в процессе обучения, что эквивалентно увеличению эффективного темпа обучения. Также стоит отметить тенденцию к уменьшению нормы эффективного градиента по мере перемещения из точки 1 (точность на обучении равна 100\%) в точку 2 (точность на обучении и на валидации равна 100\%), что является подтверждением перемещения по мере обучения в минимум, обладающий лучшей генерализацией (норма стохастического градиента является метрикой, по которой довольно грубо можно оценить <<качество>> минимума: чем меньше норма, тем лучшей генерализацией он обладает \cite{sharpness, loss_surfaces}).
\section{Выводы}
На основании поставленных в процессе исследования экспериментов можно сделать следующие выводы о возникновении периодического поведения в процессе обучения глубоких нейронных сетей:
\begin{enumerate}
    \item Периодическое поведение возникает в процессе обучения масштаб-инвариантных нейросетей с использованием регуляризатора, в частности weight decay.
    \item Масштаб-инвариантности нейросети (частичной масштаб-инвариантности) можно добиться путём использования слоёв нормализации (в частности \verb|LayerNorm|) между каждыми двумя слоями модели.
    \item Периодическое поведение возникает за счёт противодействия в процессе обучения двух движущих сил:
    weight decay, стремящегося уменьшить норму весов, и градиента, за счёт которого происходит перемещение по гиперсферам в пространстве весов модели.
    \item Для корректного оценивания статистик обучения масштаб-инвариантной модели следует использовать эффективные градиент и темп обучения.
\end{enumerate}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
\cite{grokking_start}
\cite{batch_norm, layer_norm}
 Например, в статье  \cite{on_periodic_beh} описываются результаты использования слоёв нормализации \verb|BatchNorm| вместе с регуляризатором <<weigth decay>> и возникающее в процессе обучения нейросети периодическое поведение.
 \cite{omnigrok, si_attn}
