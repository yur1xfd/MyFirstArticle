@article{grokking_start,
  title={Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets},
  author={Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra},
  journal={arXiv:2201.02177},
  year={2022}
}
@article{on_periodic_beh,
  title={On the Periodic Behavior of Neural Network Training with Batch Normalization and Weight Decay},
  author={Ekaterina Lobacheva, Maxim Kodryan, Nadezhda Chirkova, Andrey Malinin, Dmitry Vetrov},
  journal={arXiv:2106.15739},
  year={2021}
}
@article{batch_norm,
  title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author={Sergey Ioffe, Christian Szegedy},
  journal={arXiv:1502.03167},
  year={2015}
}
@article{layer_norm,
  title={Layer Normalization},
  author={Jimmy Lei Ba, Jamie Ryan Kirosa, Geoffrey E. Hinton},
  journal={arXiv:1607.06450},
  year={2016}
}

@article{si_attn,
  title={Robust Training of Neural Networks Using Scale Invariant Architectures},
  author={Zhiyuan Li, Srinadh Bhojanapalli, Manzil Zaheer, Sashank J. Reddi, Sanjiv Kumar},
  journal={arXiv:2202.00980},
  year={2023}
}

@article{double_descent,
  title={Deep Double Descent: Where Bigger Models and More Data Hurt},
  author={Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, Ilya Sutskever},
  journal={arXiv:1912.02292},
  year={2019}
}

@article{dd_avoid,
  title={Can we avoid Double Descent in Deep Neural Networks?},
  author={Victor Qu√©tu, Enzo Tartaglione},
  journal={arXiv:2302.13259},
  year={2023}
}

@article{omnigrok,
  title={Omnigrok: Grokking Beyond Algorithmic Data},
  author={Ziming Liu, Eric J. Michaud, Max Tegmark},
  journal={arXiv:2210.01117},
  year={2022}
}

@article{attn_is_all,
  title={Attention Is All You Need},
  author={Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin},
  journal={arXiv:1706.03762},
  year={2017}
}
