@misc{grokking_start,
      title={Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets}, 
      author={Alethea Power and Yuri Burda and Harri Edwards and Igor Babuschkin and Vedant Misra},
      year={2022},
      eprint={2201.02177},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{on_periodic_beh,
      title={On the Periodic Behavior of Neural Network Training with Batch Normalization and Weight Decay}, 
      author={Ekaterina Lobacheva and Maxim Kodryan and Nadezhda Chirkova and Andrey Malinin and Dmitry Vetrov},
      year={2021},
      eprint={2106.15739},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{batch_norm,
      title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}, 
      author={Sergey Ioffe and Christian Szegedy},
      year={2015},
      eprint={1502.03167},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{layer_norm,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{si_attn,
      title={Robust Training of Neural Networks Using Scale Invariant Architectures}, 
      author={Zhiyuan Li and Srinadh Bhojanapalli and Manzil Zaheer and Sashank J. Reddi and Sanjiv Kumar},
      year={2022},
      eprint={2202.00980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{double_descent,
      title={Deep Double Descent: Where Bigger Models and More Data Hurt}, 
      author={Preetum Nakkiran and Gal Kaplun and Yamini Bansal and Tristan Yang and Boaz Barak and Ilya Sutskever},
      year={2019},
      eprint={1912.02292},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{dd_avoid,
      title={Can we avoid Double Descent in Deep Neural Networks?}, 
      author={Victor Qu√©tu and Enzo Tartaglione},
      year={2023},
      eprint={2302.13259},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{omnigrok,
      title={Omnigrok: Grokking Beyond Algorithmic Data}, 
      author={Ziming Liu and Eric J. Michaud and Max Tegmark},
      year={2023},
      eprint={2210.01117},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{attn_is_all,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{weight_decay,
      title={On the training dynamics of deep networks with $L_2$ regularization}, 
      author={Aitor Lewkowycz and Guy Gur-Ari},
      year={2020},
      eprint={2006.08643},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{decoupled_wd,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{grokking_tickets,
      title={Grokking Tickets: Lottery Tickets Accelerate Grokking}, 
      author={Gouki Minegishi and Yusuke Iwasawa and Yutaka Matsuo},
      year={2023},
      eprint={2310.19470},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{lazy_grokking,
      title={Grokking as the Transition from Lazy to Rich Training Dynamics}, 
      author={Tanishq Kumar and Blake Bordelon and Samuel J. Gershman and Cengiz Pehlevan},
      year={2023},
      eprint={2310.06110},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{phase_changes,
      title={Progress measures for grokking via mechanistic interpretability}, 
      author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
      year={2023},
      eprint={2301.05217},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{gradient_reg,
      title={Understanding Gradient Regularization in Deep Learning: Efficient Finite-Difference Computation and Implicit Bias}, 
      author={Ryo Karakida and Tomoumi Takase and Tomohiro Hayase and Kazuki Osawa},
      year={2023},
      eprint={2210.02720},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{sam,
      title={Sharpness-Aware Minimization for Efficiently Improving Generalization}, 
      author={Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
      year={2021},
      eprint={2010.01412},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{asam,
      title={ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks}, 
      author={Jungmin Kwon and Jeongseop Kim and Hyunseo Park and In Kwon Choi},
      year={2021},
      eprint={2102.11600},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{lion,
      title={Symbolic Discovery of Optimization Algorithms}, 
      author={Xiangning Chen and Chen Liang and Da Huang and Esteban Real and Kaiyuan Wang and Yao Liu and Hieu Pham and Xuanyi Dong and Thang Luong and Cho-Jui Hsieh and Yifeng Lu and Quoc V. Le},
      year={2023},
      eprint={2302.06675},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{loss_surfaces,
      title={Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs}, 
      author={Timur Garipov and Pavel Izmailov and Dmitrii Podoprikhin and Dmitry Vetrov and Andrew Gordon Wilson},
      year={2018},
      eprint={1802.10026},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{sharpness,
      title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}, 
      author={Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang},
      year={2017},
      eprint={1609.04836},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{3_rigimes,
      title={Training Scale-Invariant Neural Networks on the Sphere Can Happen in Three Regimes}, 
      author={Maxim Kodryan and Ekaterina Lobacheva and Maksim Nakhodnov and Dmitry Vetrov},
      year={2023},
      eprint={2209.03695},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
